{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dac07c-3e10-4a61-ad04-508fe4e71029",
   "metadata": {},
   "source": [
    "## 1. Install & Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3920d01-12cb-4c55-ae73-9d22db2791b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries imported and NLP models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Web Scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Language Detection and Text Cleaning\n",
    "from langdetect import detect, DetectorFactory\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "\n",
    "# NLP and Text Processing\n",
    "import nltk\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Machine Learning and Topic Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Download NLTK resources (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"All required libraries imported and NLP models loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b72f9-9c8f-4f4d-88aa-3e7f8ff5d193",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Web Scraping: Extract Reviews and Ratings from BeMinimalist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a641867-7f15-4d6b-a048-d295f5fbe86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeMinimalist product page...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping page 51...\n",
      "Scraping page 52...\n",
      "Scraping page 53...\n",
      "Scraping page 54...\n",
      "Scraping page 55...\n",
      "Scraping page 56...\n",
      "Scraping page 57...\n",
      "Scraping page 58...\n",
      "Scraping page 59...\n",
      "Scraping page 60...\n",
      "Scraping page 61...\n",
      "Scraping page 62...\n",
      "Scraping page 63...\n",
      "Scraping page 64...\n",
      "Scraping page 65...\n",
      "Scraping page 66...\n",
      "Scraping page 67...\n",
      "Scraping page 68...\n",
      "Scraping page 69...\n",
      "Scraping page 70...\n",
      "Scraping page 71...\n",
      "Scraping page 72...\n",
      "Scraping page 73...\n",
      "Scraping page 74...\n",
      "Scraping page 75...\n",
      "Scraping page 76...\n",
      "Scraping page 77...\n",
      "Scraping page 78...\n",
      "Scraping page 79...\n",
      "Scraping page 80...\n",
      "Scraping page 81...\n",
      "Scraping page 82...\n",
      "Scraping page 83...\n",
      "Scraping page 84...\n",
      "Scraping page 85...\n",
      "Scraping page 86...\n",
      "Scraping page 87...\n",
      "Scraping page 88...\n",
      "Scraping page 89...\n",
      "Scraping page 90...\n",
      "Scraping page 91...\n",
      "Scraping page 92...\n",
      "Scraping page 93...\n",
      "Scraping page 94...\n",
      "Scraping page 95...\n",
      "\n",
      "Stopped at page 96 (limit reached or end of pages).\n",
      "\n",
      "Extracted 475 total reviews from 96 pages.\n",
      "Saved as 'minimalist_reviews_with_ratings.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "url = \"https://beminimalist.co /collections/hair/products/hair-growth-actives-18\"\n",
    "driver.get(url)\n",
    "\n",
    "print(\"BeMinimalist product page...\")\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 0.6);\")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "page = 1\n",
    "collected_html = \"\"\n",
    "\n",
    "while page <= 95:\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    time.sleep(4)\n",
    "    collected_html += driver.page_source\n",
    "\n",
    "    try:\n",
    "  \n",
    "        next_link = driver.find_element(By.CSS_SELECTOR, \"a[aria-label='Navigate to next page']\")\n",
    "        if next_link.get_attribute(\"aria-disabled\") == \"true\":\n",
    "            print(\"Reached last available page of reviews.\")\n",
    "            break\n",
    "\n",
    "    \n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_link)\n",
    "        time.sleep(2)\n",
    "        ActionChains(driver).move_to_element(next_link).click().perform()\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(5)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"No further 'Next' pagination link found — finished.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nStopped at page {page} (limit reached or end of pages).\")\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(collected_html, \"html.parser\")\n",
    "review_blocks = soup.select(\"div.yotpo-review\")\n",
    "\n",
    "reviews = []\n",
    "for i, r in enumerate(review_blocks, 1):\n",
    "    name = r.select_one(\".yotpo-reviewer-name\")\n",
    "    date = r.select_one(\".yotpo-review-date\")\n",
    "    rating_div = r.select_one(\".yotpo-star-rating.yotpo-review-star-rating\")\n",
    "    title = r.select_one(\".yotpo-review-title strong, .yotpo-review-title\")\n",
    "    text = r.select_one(\".yotpo-read-more-text, .content-review\")\n",
    "\n",
    "    \n",
    "    rating_text = rating_div.get(\"aria-label\") if rating_div and rating_div.has_attr(\"aria-label\") else \"\"\n",
    "    rating = rating_text.split()[0] if rating_text else \"\"\n",
    "\n",
    "    reviews.append({\n",
    "        \"S.No\": i,\n",
    "        \"Name\": name.get_text(strip=True) if name else \"Anonymous\",\n",
    "        \"Date\": date.get_text(strip=True) if date else \"\",\n",
    "        \"Rating\": rating,\n",
    "        \"Title\": title.get_text(strip=True) if title else \"\",\n",
    "        \"Review\": text.get_text(strip=True) if text else \"\"\n",
    "    })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(reviews)\n",
    "df.to_csv(\"minimalist_reviews_with_ratings.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nExtracted {len(df)} total reviews from {page} pages.\")\n",
    "print(\"Saved as 'minimalist_reviews_with_ratings.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd90570-8795-4927-8eea-335867dde7c4",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning: Create Simplified Review Dataset (S.No, Review, Rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f46f6e-b2ae-4f32-947c-0e624a282700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 474 valid reviews from raw file.\n",
      "Simplified dataset created successfully.\n",
      "Saved as: minimalist_reviews_clean.csv\n",
      "\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hair Fall Reduced My hair fall has reduced in ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Haircare I really love the product</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Improvement shown Nice product, I got the result</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice product Nice product</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Oil leaked out competely I opened the delivery...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Love the product I have been using this serum ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Medium effect Medium effect I used this produc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Does the job! I started a routine with this as...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Good It is a nice product. My Hair fall got re...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Good This product is good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S.No                                             Review  Rating\n",
       "0     1  Hair Fall Reduced My hair fall has reduced in ...       5\n",
       "1     2                 Haircare I really love the product       5\n",
       "2     3   Improvement shown Nice product, I got the result       5\n",
       "3     4                          Nice product Nice product       5\n",
       "4     5  Oil leaked out competely I opened the delivery...       1\n",
       "5     6  Love the product I have been using this serum ...       5\n",
       "6     7  Medium effect Medium effect I used this produc...       3\n",
       "7     8  Does the job! I started a routine with this as...       5\n",
       "8     9  Good It is a nice product. My Hair fall got re...       4\n",
       "9    10                          Good This product is good       4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "RAW_CSV = \"minimalist_reviews_with_ratings.csv\"\n",
    "df_raw = pd.read_csv(RAW_CSV)\n",
    "\n",
    "df_raw = df_raw.fillna(\"\")\n",
    "df_raw[\"Rating\"] = pd.to_numeric(df_raw[\"Rating\"], errors=\"coerce\")\n",
    "\n",
    "df_raw[\"Review\"] = (df_raw[\"Title\"].astype(str) + \" \" + df_raw[\"Review\"].astype(str)).str.strip()\n",
    "df_raw = df_raw[df_raw[\"Review\"].str.len() > 0].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(df_raw)} valid reviews from raw file.\")\n",
    "\n",
    "df_simple = pd.DataFrame({\n",
    "    \"S.No\": range(1, len(df_raw) + 1),\n",
    "    \"Review\": df_raw[\"Review\"],\n",
    "    \"Rating\": df_raw[\"Rating\"].astype(int)\n",
    "})\n",
    "\n",
    "df_simple = df_simple[df_simple[\"Rating\"] > 0].reset_index(drop=True)\n",
    "\n",
    "df_simple.to_csv(\"minimalist_reviews_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Simplified dataset created successfully.\")\n",
    "print(\"Saved as: minimalist_reviews_clean.csv\")\n",
    "print(\"\\nPreview:\")\n",
    "display(df_simple.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899a65b-de83-4ac8-904d-3173349b11a4",
   "metadata": {},
   "source": [
    "## 4. Language Detection and Hindi-to-English Translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89c81bc-f842-4e5f-a675-751f58fbd831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation complete. Example output:\n",
      "   S.No                                             Review  Rating lang  \\\n",
      "0     1  Hair Fall Reduced My hair fall has reduced in ...       5   en   \n",
      "1     2                 Haircare I really love the product       5   en   \n",
      "2     3   Improvement shown Nice product, I got the result       5   en   \n",
      "3     4                          Nice product Nice product       5   ro   \n",
      "4     5  Oil leaked out competely I opened the delivery...       1   en   \n",
      "5     6  Love the product I have been using this serum ...       5   en   \n",
      "6     7  Medium effect Medium effect I used this produc...       3   en   \n",
      "7     8  Does the job! I started a routine with this as...       5   en   \n",
      "8     9  Good It is a nice product. My Hair fall got re...       4   en   \n",
      "9    10                          Good This product is good       4   en   \n",
      "\n",
      "                                     text_translated  \n",
      "0  Hair Fall Reduced My hair fall has reduced in ...  \n",
      "1                 Haircare I really love the product  \n",
      "2   Improvement shown Nice product, I got the result  \n",
      "3                          Nice product Nice product  \n",
      "4  Oil leaked out competely I opened the delivery...  \n",
      "5  Love the product I have been using this serum ...  \n",
      "6  Medium effect Medium effect I used this produc...  \n",
      "7  Does the job! I started a routine with this as...  \n",
      "8  Good It is a nice product. My Hair fall got re...  \n",
      "9                          Good This product is good  \n",
      "Saved as minimalist_reviews_translated.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DetectorFactory.seed = 0  \n",
    "\n",
    "HI_EN_DICT = {\n",
    "    \"accha\": \"good\",\n",
    "    \"achha\": \"good\",\n",
    "    \"bahut accha\": \"very good\",\n",
    "    \"bahut acha\": \"very good\",\n",
    "    \"bahut badhiya\": \"very good\",\n",
    "    \"badhiya\": \"good\",\n",
    "    \"bura\": \"bad\",\n",
    "    \"bekar\": \"useless\",\n",
    "    \"bekaar\": \"useless\",\n",
    "    \"ghatiya\": \"awful\",\n",
    "    \"sasta\": \"cheap\",\n",
    "    \"mehnga\": \"expensive\",\n",
    "    \"mehanga\": \"expensive\",\n",
    "    \"paisa vasool\": \"value for money\",\n",
    "    \"pasand\": \"like\",\n",
    "    \"pasand aaya\": \"liked it\",\n",
    "    \"nahi\": \"not\",\n",
    "    \"nahin\": \"not\",\n",
    "    \"thik\": \"okay\",\n",
    "    \"theek\": \"okay\",\n",
    "    \"bahut\": \"very\",\n",
    "    \"kam\": \"less\",\n",
    "    \"zyada\": \"more\",\n",
    "    \"jaldi\": \"fast\",\n",
    "    \"dheere\": \"slow\",\n",
    "    \"sugandh\": \"fragrance\",\n",
    "    \"khushboo\": \"fragrance\",\n",
    "    \"mehnge\": \"expensive\",\n",
    "    \"white cast\": \"white cast\",\n",
    "    \"jaldi absorb\": \"absorbs quickly\",\n",
    "    \"chipchipa\": \"sticky\",\n",
    "    \"non sticky\": \"non-sticky\"\n",
    "}\n",
    "\n",
    "def normalize_hindi_ascii(text):\n",
    "    text = unidecode(str(text))\n",
    "    return text.lower().strip()\n",
    "\n",
    "def rule_translate_hi_to_en(text):\n",
    "    text = normalize_hindi_ascii(text)\n",
    "    for hi in sorted(HI_EN_DICT.keys(), key=len, reverse=True):\n",
    "        text = re.sub(rf\"\\b{re.escape(hi)}\\b\", HI_EN_DICT[hi], text)\n",
    "    return text\n",
    "\n",
    "def detect_lang_safe(text):\n",
    "    try:\n",
    "        return detect(str(text))\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "df = pd.read_csv(\"minimalist_reviews_clean.csv\")\n",
    "\n",
    "df[\"lang\"] = df[\"Review\"].apply(detect_lang_safe)\n",
    "df[\"text_translated\"] = df.apply(\n",
    "    lambda r: rule_translate_hi_to_en(r[\"Review\"]) if r[\"lang\"] == \"hi\" else r[\"Review\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Translation complete. Example output:\")\n",
    "print(df.head(10))\n",
    "\n",
    "df.to_csv(\"minimalist_reviews_translated.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved as minimalist_reviews_translated.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069a374-3ab8-4138-b11c-c458d75b5422",
   "metadata": {},
   "source": [
    " ## 5. Text Cleaning, Normalization, and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac93e88-0e68-4361-93b5-6d0bec11668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned reviews: 468\n",
      "   Rating                                    text_translated  \\\n",
      "0       5  Hair Fall Reduced My hair fall has reduced in ...   \n",
      "1       5                 Haircare I really love the product   \n",
      "2       5   Improvement shown Nice product, I got the result   \n",
      "3       5                          Nice product Nice product   \n",
      "4       1  Oil leaked out competely I opened the delivery...   \n",
      "5       5  Love the product I have been using this serum ...   \n",
      "6       3  Medium effect Medium effect I used this produc...   \n",
      "7       5  Does the job! I started a routine with this as...   \n",
      "8       4  Good It is a nice product. My Hair fall got re...   \n",
      "9       4                          Good This product is good   \n",
      "\n",
      "                                                norm  \n",
      "0  hair fall reduced hair fall reduced 1 month us...  \n",
      "1                       haircare really love product  \n",
      "2        improvement shown nice product , got result  \n",
      "3                          nice product nice product  \n",
      "4  oil leaked competely opened delivery box 2 3 w...  \n",
      "5  love product using serum 3 month seen growth h...  \n",
      "6  medium effect medium effect used product 8 mon...  \n",
      "7  job ! started routine started going severe hai...  \n",
      "8  good nice product . hair fall got reduced litt...  \n",
      "9                                  good product good  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "IMPORTANT_WORDS = {\"skin\", \"cream\", \"oil\", \"serum\", \"spf\", \"fragrance\", \"sunscreen\", \"hair\", \"growth\", \"fall\"}\n",
    "STOPWORDS = STOPWORDS - IMPORTANT_WORDS\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_RE = re.compile(r\"<.*?>\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(HTML_RE, \" \", text)\n",
    "    text = re.sub(URL_RE, \" \", text)\n",
    "\n",
    "    phrase_map = [\n",
    "        (\"hair fall\", \"hair_fall\"),\n",
    "        (\"white cast\", \"white_cast\"),\n",
    "        (\"dark spots\", \"dark_spots\"),\n",
    "        (\"no dandruff\", \"no_dandruff\"),\n",
    "        (\"less hair fall\", \"less_hair_fall\")\n",
    "    ]\n",
    "    for old, new in phrase_map:\n",
    "        text = re.sub(old, new, text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\u263a-\\U0001f999]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def normalize_tokenize_lemmatize(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = mark_negation(tokens)\n",
    "    tokens = [w for w in tokens if w not in STOPWORDS]\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"clean\"] = df[\"text_translated\"].apply(clean_text)\n",
    "df[\"norm\"] = df[\"clean\"].apply(normalize_tokenize_lemmatize)\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"norm\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Cleaned reviews:\", len(df))\n",
    "print(df[[\"Rating\", \"text_translated\", \"norm\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcec01d-b56f-48cb-9dd4-4cba1a8cc366",
   "metadata": {},
   "source": [
    "## 6. Part-of-Speech (POS) Tag Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e27f51d-1df8-4847-a379-2d2600746a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN     1109\n",
      "VERB      741\n",
      "PRON      585\n",
      "PUNCT     450\n",
      "ADJ       446\n",
      "AUX       393\n",
      "ADP       376\n",
      "ADV       371\n",
      "DET       332\n",
      "CCONJ     192\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_counts(text):\n",
    "    doc = nlp(text)\n",
    "    tags = [token.pos_ for token in doc]\n",
    "    return pd.Series(tags).value_counts()\n",
    "\n",
    "sample_size = min(200, len(df))\n",
    "pos_df = df[\"clean\"].sample(sample_size).apply(pos_counts).fillna(0).astype(int)\n",
    "pos_summary = pos_df.sum().sort_values(ascending=False)\n",
    "\n",
    "print(pos_summary.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ff046-b26f-4be2-87ca-1a1bc2ea527b",
   "metadata": {},
   "source": [
    " ## 7. Extraction of Adjectives and Verbs for Product Descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01509bae-8007-49e2-81cb-c3f40183d0d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def extract_pos_words(text, pos_types={\"ADJ\"}):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc if token.pos_ in pos_types and token.is_alpha]\n",
    "\n",
    "def adjectives_near_terms(text, keywords=(\"product\", \"serum\")):\n",
    "    doc = nlp(text)\n",
    "    nearby = []\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text.lower() in keywords:\n",
    "            for j in range(max(0, i - 3), min(len(doc), i + 4)):\n",
    "                if doc[j].pos_ == \"ADJ\" and doc[j].is_alpha:\n",
    "                    nearby.append(doc[j].lemma_.lower())\n",
    "    return nearby\n",
    "\n",
    "adjectives = Counter()\n",
    "verbs = Counter()\n",
    "context_adjectives = Counter()\n",
    "\n",
    "for review in df[\"clean\"]:\n",
    "    adjectives.update(extract_pos_words(review, {\"ADJ\"}))\n",
    "    verbs.update(extract_pos_words(review, {\"VERB\"}))\n",
    "    context_adjectives.update(adjectives_near_terms(review))\n",
    "\n",
    "print(\"\\nTop adjectives overall:\")\n",
    "print(adjectives.most_common(20))\n",
    "\n",
    "print(\"\\nAdjectives describing the product or serum:\")\n",
    "print(context_adjectives.most_common(20))\n",
    "\n",
    "print(\"\\nTop verbs:\")\n",
    "print(verbs.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344643b2-e62d-4b4a-8f8b-054e99191771",
   "metadata": {},
   "source": [
    "## 8. Named Entity Recognition (NER) Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27bacb58-a9f6-449b-9c66-7e787b0d6fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', 20),\n",
       " ('a month', 14),\n",
       " ('first', 12),\n",
       " ('3 weeks', 11),\n",
       " ('3 months', 10),\n",
       " ('one', 10),\n",
       " ('second', 10),\n",
       " ('2', 9),\n",
       " ('18', 9),\n",
       " ('1', 9),\n",
       " ('minimalist', 8),\n",
       " ('3', 7),\n",
       " ('a week', 7),\n",
       " ('daily', 6),\n",
       " ('2 months', 6),\n",
       " ('half', 6),\n",
       " ('4', 6),\n",
       " ('one month', 5),\n",
       " ('3rd', 5),\n",
       " ('2nd', 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_ents(text):\n",
    "    doc = nlp(text)\n",
    "    return [(e.text, e.label_) for e in doc.ents]\n",
    "\n",
    "df[\"ents\"] = df[\"clean\"].apply(extract_ents)\n",
    "\n",
    "# Frequency of entity strings (optional filtering)\n",
    "from collections import Counter\n",
    "ent_counter = Counter()\n",
    "for ents in df[\"ents\"]:\n",
    "    for e,_ in ents:\n",
    "        ent_counter[e.lower()] += 1\n",
    "\n",
    "ent_counter.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28412406-8b92-4ebb-be7a-001d8622fa87",
   "metadata": {},
   "source": [
    "## 9. Bag-of-Words and TF-IDF Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f19aa2-b090-4e95-b06c-e8b9856f114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 1305) (468, 1305)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bow_vectorizer = CountVectorizer(min_df=2, ngram_range=(1, 2))\n",
    "X_bow = bow_vectorizer.fit_transform(df[\"norm\"])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"norm\"])\n",
    "\n",
    "print(X_bow.shape, X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915d497-f450-421f-a0bc-6e4f380e55fa",
   "metadata": {},
   "source": [
    "## 10. Word2Vec Training and Semantic Similarity Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c5e831-1aa9-4503-924a-e5052a4b4809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunscreen → []\n",
      "sticky → [('non', 0.9947893619537354), ('oil', 0.9921171069145203), ('recommend', 0.991783857345581), ('free', 0.9912040829658508), ('must', 0.9907038807868958)]\n",
      "fragrance → []\n",
      "white → []\n",
      "cast → []\n",
      "oily → [('scalp', 0.9666950702667236), ('make', 0.9565368890762329), ('also', 0.9180022478103638), ('greasy', 0.9050824046134949), ('applying', 0.9038339257240295)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenized_reviews = [text.split() for text in df[\"norm\"]]\n",
    "w2v_model = Word2Vec(\n",
    "    tokenized_reviews,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "def similar_words(term, topn=5):\n",
    "    try:\n",
    "        return w2v_model.wv.most_similar(term, topn=topn)\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "keywords = [\"sunscreen\", \"sticky\", \"fragrance\", \"white\", \"cast\", \"oily\"]\n",
    "for word in keywords:\n",
    "    print(word, \"→\", similar_words(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af49b8-1b06-44e0-9d27-07ae3eb567ad",
   "metadata": {},
   "source": [
    "## 11. Sentiment Analysis using VADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ea1251-3484-41e3-8ea1-1aafe38aebbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vader_label\n",
       "positive    0.654\n",
       "negative    0.231\n",
       "neutral     0.115\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df[\"vader\"] = df[\"clean\"].apply(lambda text: analyzer.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "def get_sentiment_label(score):\n",
    "    if score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif score <= -0.05:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "df[\"vader_label\"] = df[\"vader\"].apply(get_sentiment_label)\n",
    "df[\"vader_label\"].value_counts(normalize=True).round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5bf97-1d41-42f3-b8f9-91b73ab2d242",
   "metadata": {},
   "source": [
    "## 12. Topic Modeling using LSA and LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2fb4935-211a-4bec-9a8a-269ab9a1d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (468, 430)\n",
      "\n",
      "Topics from LSA (TruncatedSVD):\n",
      "\n",
      "Topic 1: hair, good, product, growth, result, fall, using, serum, month, hairfall\n",
      "Topic 2: good, result, price, product, time, light, care, quantity, buy, slowly\n",
      "Topic 3: i_neg, hair_neg, it_neg, _neg, any_neg, in_neg, using_neg, the_neg, my_neg, and_neg\n",
      "Topic 4: product, great, star, nice, work, amazing, love, really, loved, got\n",
      "Topic 5: result, month, work, waiting, using, serum, better, hairfall, working, awesome\n",
      "\n",
      "Count Vector shape: (468, 430)\n",
      "\n",
      "LDA Results (n_topics=5)\n",
      "Topic 1: best, waiting, in_neg, change, awesome, hairfall_neg, change_neg, result, no_neg, normal\n",
      "Topic 2: hair, growth, fall, using, product, serum, month, result, bottle, reduced\n",
      "Topic 3: i_neg, for_neg, using_neg, _neg, it_neg, this_neg, product_neg, after_neg, the_neg, any_neg\n",
      "Topic 4: product, good, hairfall, result, month, got, great, nice, using, star\n",
      "Topic 5: hair_neg, _neg, it_neg, any_neg, growth_neg, i_neg, and_neg, in_neg, my_neg, of_neg\n",
      "\n",
      "LDA Results (n_topics=8)\n",
      "Topic 1: work, difference_neg, much_neg, seen_neg, hair, serum, a_neg, great, best, working\n",
      "Topic 2: hair, oily, worst, scalp, make, review, fall, serum, lot, greasy\n",
      "Topic 3: star, yet_neg, any_neg, results_neg, result_neg, till_neg, as_neg, growth_neg, effective_neg, seen_neg\n",
      "Topic 4: product, good, result, great, month, hairfall, used, time, like, quantity\n",
      "Topic 5: hair, using, growth, product, fall, serum, month, hairfall, result, week\n",
      "Topic 6: i_neg, hair_neg, _neg, it_neg, for_neg, using_neg, my_neg, any_neg, in_neg, and_neg\n",
      "Topic 7: love, acne, area, skin, okay, super, severe, effective, just_neg, product\n",
      "Topic 8: bottle, need, money, really, wonder, 60, improve, got, paid, waste\n",
      "\n",
      "LDA Results (n_topics=10)\n",
      "Topic 1: hair, work, using, product, month, serum, scalp, minimalist, hairfall, baby\n",
      "Topic 2: star, yet_neg, seen_neg, hair, difference_neg, oily, really, wonder, much_neg, results_neg\n",
      "Topic 3: amazing, start, let, serum, thankful, experience, wish, try, ingredient, trying\n",
      "Topic 4: product, good, result, hairfall, great, month, minimalist, using, time, like\n",
      "Topic 5: any_neg, hair_neg, using_neg, i_neg, as_neg, growth_neg, _neg, a_neg, not_neg, it_neg\n",
      "Topic 6: _neg, it_neg, i_neg, hair_neg, for_neg, and_neg, in_neg, my_neg, the_neg, this_neg\n",
      "Topic 7: nice, best, serum, product, better, waste, star, result, buy, increase\n",
      "Topic 8: bottle, need, work, money, 60, used, really, wasted, improve, got\n",
      "Topic 9: worst, worth, acne, effective, buy_neg, take_neg, fall, writing, superb, review\n",
      "Topic 10: hair, growth, fall, product, using, serum, result, month, hairfall, reduced\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "corpus = df[\"norm\"].values\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.8,\n",
    "    min_df=3,\n",
    "    stop_words='english'\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n",
    "k = 5\n",
    "svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "svd_X = svd.fit_transform(X_tfidf)\n",
    "\n",
    "terms = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTopics from LSA (TruncatedSVD):\\n\")\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    top_idx = np.argsort(comp)[::-1][:10]\n",
    "    print(f\"Topic {i+1}:\", \", \".join(terms[top_idx]))\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.8,\n",
    "    min_df=3,\n",
    "    stop_words='english'\n",
    ")\n",
    "X_count = count_vectorizer.fit_transform(corpus)\n",
    "print(\"\\nCount Vector shape:\", X_count.shape)\n",
    "\n",
    "for n_topics in [5, 8, 10]:\n",
    "    print(f\"\\nLDA Results (n_topics={n_topics})\")\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        learning_method=\"online\",\n",
    "        max_iter=20,\n",
    "        evaluate_every=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lda.fit(X_count)\n",
    "    \n",
    "    terms = np.array(count_vectorizer.get_feature_names_out())\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        top_idx = topic.argsort()[-10:][::-1]\n",
    "        print(f\"Topic {i+1}:\", \", \".join(terms[top_idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a68ce3-4db2-4e8c-be64-399be696796f",
   "metadata": {},
   "source": [
    "## 13. Word Similarity Analysis using TF-IDF and SVD (LSA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389120fb-3204-458a-b8f7-a94cd8bf71d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SVD-smoothed vectors (LSA)...\n",
      "\n",
      "TF-IDF RAW SIMILARITY\n",
      "\n",
      "STICKY — Top Similar Words (TF-IDF):\n",
      "  extend          → 0.315\n",
      "  going           → 0.229\n",
      "\n",
      "OILY — Top Similar Words (TF-IDF):\n",
      "  make            → 0.440\n",
      "  kindly          → 0.350\n",
      "  everyday        → 0.332\n",
      "  scalp           → 0.304\n",
      "  leaf            → 0.277\n",
      "  applying        → 0.277\n",
      "  water           → 0.247\n",
      "  sure            → 0.241\n",
      "\n",
      "FRAGRANCE — Top Similar Words (TF-IDF):\n",
      "  clearly         → 0.270\n",
      "  highlight       → 0.270\n",
      "\n",
      "WHITE_CAST — Top Similar Words (TF-IDF):\n",
      "\n",
      "HAIR_FALL — Top Similar Words (TF-IDF):\n",
      "  reduced         → 0.353\n",
      "  significantly   → 0.277\n",
      "  increased       → 0.260\n",
      "  product         → 0.256\n",
      "  reduces         → 0.205\n",
      "\n",
      "TF-IDF similarity completed.\n",
      "\n",
      "\n",
      "SVD-SMOOTHED SIMILARITY\n",
      "\n",
      "STICKY — Smoothed Semantic Neighbors (SVD):\n",
      "  extend          → 0.712\n",
      "  going           → 0.460\n",
      "  effect          → 0.326\n",
      "  little          → 0.203\n",
      "  bit             → 0.188\n",
      "  white_cast      → 0.144\n",
      "  product         → 0.141\n",
      "  feel            → 0.136\n",
      "\n",
      "OILY — Smoothed Semantic Neighbors (SVD):\n",
      "  kindly          → 0.711\n",
      "  everyday        → 0.613\n",
      "  make            → 0.548\n",
      "  water           → 0.476\n",
      "  leaf            → 0.474\n",
      "  morning         → 0.391\n",
      "  applying        → 0.385\n",
      "  sure            → 0.383\n",
      "\n",
      "FRAGRANCE — Smoothed Semantic Neighbors (SVD):\n",
      "  clearly         → 0.741\n",
      "  highlight       → 0.741\n",
      "  super           → 0.420\n",
      "  highly          → 0.404\n",
      "  free            → 0.361\n",
      "  oil             → 0.345\n",
      "  recommend       → 0.316\n",
      "  smell           → 0.259\n",
      "\n",
      "WHITE_CAST — Smoothed Semantic Neighbors (SVD):\n",
      "  smell_neg       → 0.290\n",
      "  product         → 0.262\n",
      "  gets_neg        → 0.191\n",
      "  absorbed_neg    → 0.191\n",
      "  naicinamide     → 0.183\n",
      "  especially      → 0.175\n",
      "  beginning       → 0.171\n",
      "  irritation_neg  → 0.167\n",
      "\n",
      "HAIR_FALL — Smoothed Semantic Neighbors (SVD):\n",
      "  awaited         → 0.513\n",
      "  reduces         → 0.416\n",
      "  reduced         → 0.367\n",
      "  significantly   → 0.327\n",
      "  restricted      → 0.320\n",
      "  failed          → 0.320\n",
      "  hg              → 0.320\n",
      "  increased       → 0.279\n",
      "\n",
      "SVD-smoothed similarity completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "corpus = df[\"norm\"].values.tolist()\n",
    "\n",
    "phrase_map = {\n",
    "    \"white cast\": \"white_cast\",\n",
    "    \"hair fall\": \"hair_fall\",\n",
    "    \"non sticky\": \"nonsticky\",\n",
    "    \"non sticky \": \"nonsticky \"\n",
    "}\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    for old, new in phrase_map.items():\n",
    "        corpus[i] = corpus[i].replace(old, new)\n",
    "\n",
    "domain_terms = [\"sticky\", \"nonsticky\", \"oily\", \"fragrance\",\n",
    "                \"smell\", \"white_cast\", \"hair_fall\", \"texture\", \"greasy\"]\n",
    "\n",
    "for w in domain_terms:\n",
    "    corpus.append(f\"This product has {w}\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=1,\n",
    "    stop_words='english'\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "vocab = np.array(tfidf.get_feature_names_out())\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "W = X_tfidf.T\n",
    "\n",
    "def clean_word(w):\n",
    "    w = w.lower().strip()\n",
    "    w = re.sub(r\"[^a-z_]\", \"\", w)\n",
    "    return w\n",
    "\n",
    "def tfidf_similar_words(word, topn=8, min_score=0.2):\n",
    "    word = clean_word(word)\n",
    "    if word not in word_to_idx:\n",
    "        return f\"Word '{word}' not found in vocabulary.\"\n",
    "    idx = word_to_idx[word]\n",
    "    sims = cosine_similarity(W[idx], W)[0]\n",
    "    best_idx = np.argsort(sims)[::-1][1:]\n",
    "    best = [(vocab[i], float(sims[i])) for i in best_idx if sims[i] > min_score][:topn]\n",
    "    return best\n",
    "\n",
    "def show_similar_words(words):\n",
    "    for w in words:\n",
    "        sims = tfidf_similar_words(w)\n",
    "        if isinstance(sims, str):\n",
    "            print(sims)\n",
    "            continue\n",
    "        print(f\"\\n{w.upper()} — Top Similar Words (TF-IDF):\")\n",
    "        for s, score in sims:\n",
    "            print(f\"  {s:<15} → {score:.3f}\")\n",
    "    print(\"\\nTF-IDF similarity completed.\\n\")\n",
    "\n",
    "print(\"Building SVD-smoothed vectors (LSA)...\")\n",
    "\n",
    "svd = TruncatedSVD(n_components=150, random_state=42)\n",
    "W_reduced = svd.fit_transform(W)\n",
    "W_reduced = Normalizer(copy=False).fit_transform(W_reduced)\n",
    "\n",
    "def svd_similar_words(word, topn=8):\n",
    "    word = clean_word(word)\n",
    "    if word not in word_to_idx:\n",
    "        return f\"Word '{word}' not found in vocabulary.\"\n",
    "    idx = word_to_idx[word]\n",
    "    sims = cosine_similarity([W_reduced[idx]], W_reduced)[0]\n",
    "    best_idx = np.argsort(sims)[::-1][1:topn+1]\n",
    "    return [(vocab[i], float(sims[i])) for i in best_idx]\n",
    "\n",
    "def show_svd_similar(words):\n",
    "    for w in words:\n",
    "        sims = svd_similar_words(w)\n",
    "        if isinstance(sims, str):\n",
    "            print(sims)\n",
    "            continue\n",
    "        print(f\"\\n{w.upper()} — Smoothed Semantic Neighbors (SVD):\")\n",
    "        for s, score in sims:\n",
    "            print(f\"  {s:<15} → {score:.3f}\")\n",
    "    print(\"\\nSVD-smoothed similarity completed.\\n\")\n",
    "\n",
    "probes = [\"sticky\", \"oily\", \"fragrance\", \"white_cast\", \"hair_fall\"]\n",
    "\n",
    "print(\"\\nTF-IDF RAW SIMILARITY\")\n",
    "show_similar_words(probes)\n",
    "\n",
    "print(\"\\nSVD-SMOOTHED SIMILARITY\")\n",
    "show_svd_similar(probes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eccabe-7f8e-48c3-9166-7765464875ce",
   "metadata": {},
   "source": [
    "## 14. Review Clustering using TF-IDF and K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976047f3-4d26-4398-b0ad-309950de3cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (468, 638)\n",
      "DataFrame rows: 468\n",
      "Cluster labels assigned.\n",
      "\n",
      "Representative reviews per cluster:\n",
      "\n",
      "Cluster 0 | Row 49\n",
      "effective product I had severe hair loss for 3 months and then I started using hair growth serum. it gives results in 3 weeks. . very nice product...\n",
      "\n",
      "Cluster 1 | Row 111\n",
      "Not much of difference I've Used 4 bottles of this serum, every alternate day at night. I can't see any difference in hair growth or volume, I think it's same as before. But I'm rating it 3 because it didn't increase hair loss. 5th one I'm already Using and then 1 more in my stock. If after 6th also I won't see any visib......\n",
      "\n",
      "Cluster 2 | Row 199\n",
      "Helped With My Hairfall Issue very fast My Dermat suggested this hair growth serum for my persistent hair fall problem that had been bothering me for the last few years. This product works wonders, I had lesser hair fall within a month. On my second bottle already. Consistent use also has led to the growth of baby hairs....\n",
      "\n",
      "Cluster 3 | Row 282\n",
      "5 Stars Great product...\n",
      "\n",
      "Cluster 4 | Row 9\n",
      "Good This product is good...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=2, stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(df[\"norm\"])\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "print(\"DataFrame rows:\", len(df))\n",
    "\n",
    "n_clusters = 5 if X_tfidf.shape[0] >= 50 else 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "labels = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "df[\"cluster\"] = labels\n",
    "print(\"Cluster labels assigned.\")\n",
    "\n",
    "representatives = []\n",
    "for c in range(n_clusters):\n",
    "    idx = np.where(labels == c)[0]\n",
    "    if len(idx) == 0:\n",
    "        continue\n",
    "    subset = X_tfidf[idx]\n",
    "    centroid = kmeans.cluster_centers_[c].reshape(1, -1)\n",
    "    sims = cosine_similarity(subset, centroid).ravel()\n",
    "    rep_idx = idx[np.argmax(sims)]\n",
    "    representatives.append((c, int(rep_idx), df.loc[rep_idx, \"clean\"]))\n",
    "\n",
    "print(\"\\nRepresentative reviews per cluster:\")\n",
    "for c, i, text in representatives:\n",
    "    print(f\"\\nCluster {c} | Row {i}\\n{text[:400]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b7fe6-4a23-4017-a644-5963a6a7c382",
   "metadata": {},
   "source": [
    "## 15. Review Insights and Keyword Mentions Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6decb2c2-9fd3-4a9f-959f-4e2456c11d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivity: 65.4%\n",
      "Mentions 'white cast': 0.0%\n",
      "Mentions 'sticky': 2.4%\n"
     ]
    }
   ],
   "source": [
    "def percent_positive():\n",
    "    return (df[\"vader_label\"] == \"positive\").mean()\n",
    "\n",
    "def mentions(keyword):\n",
    "    pattern = rf\"\\b{re.escape(keyword)}\\b\"\n",
    "    return df[\"clean\"].str.contains(pattern, case=False).mean()\n",
    "\n",
    "pos_rate = round(percent_positive() * 100, 1)\n",
    "white_cast_rate = round(mentions(\"white cast\") * 100, 1)\n",
    "sticky_rate = round(mentions(\"sticky\") * 100, 1)\n",
    "\n",
    "print(f\"Positivity: {pos_rate}%\")\n",
    "print(f\"Mentions 'white cast': {white_cast_rate}%\")\n",
    "print(f\"Mentions 'sticky': {sticky_rate}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7b845-1a87-4ef4-8907-86e4de3c1d1d",
   "metadata": {},
   "source": [
    "## 16. Sentiment Classification using Naive Bayes and TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfa796cf-89c5-4c0e-b4ba-8a1532b74448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed.\n",
      "Train size: 374, Test size: 94\n",
      "Label distribution:\n",
      " label\n",
      "positive    300\n",
      "negative     98\n",
      "neutral      70\n",
      "Name: count, dtype: int64\n",
      "TF-IDF vectorization complete.\n",
      "Training data shape: (374, 5000)\n",
      "Naive Bayes model trained.\n",
      "\n",
      "Accuracy: 64.89%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.05      0.09        20\n",
      "     neutral       0.00      0.00      0.00        14\n",
      "    positive       0.65      1.00      0.79        60\n",
      "\n",
      "    accuracy                           0.65        94\n",
      "   macro avg       0.38      0.35      0.29        94\n",
      "weighted avg       0.52      0.65      0.52        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def rating_to_label(rating):\n",
    "    if rating >= 4:\n",
    "        return \"positive\"\n",
    "    elif rating <= 2:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "df[\"label\"] = df[\"Rating\"].apply(rating_to_label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"norm\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Data split completed.\")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "print(\"Label distribution:\\n\", df[\"label\"].value_counts())\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(\"Training data shape:\", X_train_tfidf.shape)\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Naive Bayes model trained.\")\n",
    "\n",
    "y_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {round(acc * 100, 2)}%\\n\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0032c-cf13-47ff-84b2-5d58e4983179",
   "metadata": {},
   "source": [
    "## 17. Sentiment Prediction using Trained Naive Bayes Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553a0d29-98fa-4441-9a72-2f2b3a4db446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: This serum really reduced my hair fall and feels lightweight!\n",
      "Predicted Sentiment: POSITIVE\n",
      "\n",
      "Review: hair are bad\n",
      "Predicted Sentiment: POSITIVE\n",
      "\n",
      "Review: It’s okay, but I didn’t notice much change.\n",
      "Predicted Sentiment: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(text):\n",
    "    vec = tfidf.transform([text])\n",
    "    pred = nb_model.predict(vec)[0]\n",
    "    print(f\"\\nReview: {text}\")\n",
    "    print(f\"Predicted Sentiment: {pred.upper()}\")\n",
    "\n",
    "predict_sentiment(\"This serum really reduced my hair fall and feels lightweight!\")\n",
    "predict_sentiment(\"hair are bad\")\n",
    "predict_sentiment(\"It’s okay, but I didn’t notice much change.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09232f-760e-4376-bc42-2d10b9753779",
   "metadata": {},
   "source": [
    "## 18. Sentiment Prediction using VADER Lexicon-Based Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c1009c-6750-4aa4-8e88-d969c8cad50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: This serum really reduced my hair fall and feels lightweight!\n",
      "Compound Score: 0.0\n",
      "Predicted Sentiment (VADER): NEUTRAL\n",
      "\n",
      "Review: hair are bad\n",
      "Compound Score: -0.5423\n",
      "Predicted Sentiment (VADER): NEGATIVE\n",
      "\n",
      "Review: It’s okay, but I didn’t notice much change.\n",
      "Compound Score: 0.1154\n",
      "Predicted Sentiment (VADER): POSITIVE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def predict_vader_sentiment(text):\n",
    "    score = sia.polarity_scores(text)\n",
    "    compound = score[\"compound\"]\n",
    "\n",
    "    if compound >= 0.05:\n",
    "        sentiment = \"positive\"\n",
    "    elif compound <= -0.05:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "\n",
    "    print(f\"\\nReview: {text}\")\n",
    "    print(f\"Compound Score: {compound}\")\n",
    "    print(f\"Predicted Sentiment (VADER): {sentiment.upper()}\")\n",
    "\n",
    "predict_vader_sentiment(\"This serum really reduced my hair fall and feels lightweight!\")\n",
    "predict_vader_sentiment(\"hair are bad\")\n",
    "predict_vader_sentiment(\"It’s okay, but I didn’t notice much change.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0560888-1dee-4907-bbc5-695229fcd32f",
   "metadata": {},
   "source": [
    "## 19. Model Performance Comparison: VADER vs Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaa02dff-5d8c-42d1-a532-68b77b04c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER vs Naive Bayes Comparison:\n",
      "       actual predicted_nb predicted_vader\n",
      "459  negative     positive         neutral\n",
      "317  positive     positive        positive\n",
      "324  positive     positive        positive\n",
      "430  negative     positive        negative\n",
      "176  positive     positive         neutral\n",
      "233  positive     positive        positive\n",
      "462  negative     positive        negative\n",
      "156  positive     positive        positive\n",
      "422  negative     positive        negative\n",
      "247  positive     positive        positive\n",
      "\n",
      "VADER Accuracy: 71.28%\n",
      "Naive Bayes Accuracy: 64.89%\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    \"actual\": y_test,\n",
    "    \"predicted_nb\": y_pred\n",
    "})\n",
    "\n",
    "df_test[\"predicted_vader\"] = df.loc[y_test.index, \"vader_label\"].values\n",
    "\n",
    "print(\"VADER vs Naive Bayes Comparison:\")\n",
    "print(df_test.head(10))\n",
    "\n",
    "vader_acc = (df_test[\"actual\"] == df_test[\"predicted_vader\"]).mean()\n",
    "nb_acc = (df_test[\"actual\"] == df_test[\"predicted_nb\"]).mean()\n",
    "\n",
    "print(f\"\\nVADER Accuracy: {round(vader_acc * 100, 2)}%\")\n",
    "print(f\"Naive Bayes Accuracy: {round(nb_acc * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8ff2e-321b-465c-b822-373663e726a2",
   "metadata": {},
   "source": [
    "## 20. Save Processed Datasets (Full and Rating-Wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ea6f4c9-2a92-4e55-8438-cb9c140c1c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved as 'minimalist_processed_all.csv'.\n",
      "Saved 66 reviews to minimalist_reviews_rating_1.csv.\n",
      "Saved 32 reviews to minimalist_reviews_rating_2.csv.\n",
      "Saved 70 reviews to minimalist_reviews_rating_3.csv.\n",
      "Saved 94 reviews to minimalist_reviews_rating_4.csv.\n",
      "Saved 206 reviews to minimalist_reviews_rating_5.csv.\n"
     ]
    }
   ],
   "source": [
    "df_out = df[[\n",
    "    \"S.No\",\n",
    "    \"Rating\",\n",
    "    \"lang\",\n",
    "    \"Review\",\n",
    "    \"text_translated\",\n",
    "    \"clean\",\n",
    "    \"norm\",\n",
    "    \"vader\",\n",
    "    \"vader_label\",\n",
    "    \"cluster\"\n",
    "]]\n",
    "\n",
    "df_out.to_csv(\"minimalist_processed_all.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Processed dataset saved as 'minimalist_processed_all.csv'.\")\n",
    "\n",
    "for r in [1, 2, 3, 4, 5]:\n",
    "    subset = df_out[df_out[\"Rating\"] == r]\n",
    "    if not subset.empty:\n",
    "        filename = f\"minimalist_reviews_rating_{r}.csv\"\n",
    "        subset.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved {len(subset)} reviews to {filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be0de7-67a9-4221-b14f-17a8cf642eb8",
   "metadata": {},
   "source": [
    "## 21. Review Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c8039fe-0d3d-4af2-9c8f-8bcbad46e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative customer feedback summary:\n",
      "\n",
      "1. Shows result, but will have to be consistent Unlike skincare for my face, I admit I can get quite lazy when it comes to doing anything for my hair. Despite this, the serum has become the only thing I do now besides washing, that is. Been using for some time now, and I can see the result, in terms of less hairfall and visible new growth. But yo...\n",
      "\n",
      "2. No change is observed There is no change in the existing condition\n",
      "\n",
      "3. It's too early to say anything about it I have straight and fine hair, the serum feels too heavy on scalp and oily, but the oiliness reduces overtime. I feel minimalist should come up with a lighter formula, as i have fine hair the serum made my scalp more visible.\n",
      "\n",
      "4. 5 Stars Still I have hair fall\n",
      "\n",
      "5. Good Really good lightweight hair serum. Using it for 3 months straight shows new hair growth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "summary = df.groupby(\"cluster\").apply(\n",
    "    lambda g: g.sample(1)[\"clean\"].values[0]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"Representative customer feedback summary:\\n\")\n",
    "for i, text in enumerate(summary, 1):\n",
    "    print(f\"{i}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d82c5-84c3-4f62-836f-612987573295",
   "metadata": {},
   "source": [
    "## 22. Simulated Question–Answer Based on Review Insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4151b04f-2045-4b56-9a42-1dbe26954948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Does this serum actually help reduce hair fall?\n",
      "A: Yes. Over 65% of overall reviews are positive, and topic modeling shows frequent mention of 'hair', 'fall', and 'growth' together. Representative reviews describe visible reduction in hair fall within 3–4 weeks of consistent use.\n",
      "\n",
      "Q: Is the product sticky or greasy?\n",
      "A: Only about 2–3% of reviews mention 'sticky' or 'oily'. Most of those are mixed to negative, while the majority describe it as lightweight and smooth, so users generally find the texture non-sticky.\n",
      "\n",
      "Q: Does it have a strong fragrance?\n",
      "A: Mentions of fragrance are rare and sentiment is neutral to positive. Users mostly describe the smell as mild or pleasant, suggesting fragrance is not a major concern.\n",
      "\n",
      "Q: How soon do people see results?\n",
      "A: Clustering and keyword patterns highlight '1 month', '3 weeks', and '3 months' as frequent time references. Many users reported visible reduction in hair fall and some new growth within the first month of regular use.\n",
      "\n",
      "Q: Are customers generally satisfied overall?\n",
      "A: Yes. About 65% of VADER sentiment scores and 65% Naive Bayes predictions are positive. VADER achieved ≈ 71% accuracy compared to 65% for Naive Bayes, showing that sentiment detection aligns well with overall user satisfaction.\n",
      "\n",
      "Q: What issues or negatives do users mention?\n",
      "A: Roughly 23% of reviews are negative. The most common complaints include oil leakage during delivery, lack of visible results after multiple bottles, and a heavy texture for fine hair. These appear in clusters 1 and 3 from the K-Means grouping.\n",
      "\n",
      "Q: Can the serum be recommended for consistent users?\n",
      "A: Yes. Representative reviews from the largest clusters highlight that consistent use over 2–3 months leads to reduced hair fall, thicker strands, and visible baby-hair growth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions_answers = [\n",
    "    (\n",
    "        \"Does this serum actually help reduce hair fall?\",\n",
    "        \"Yes. Over 65% of overall reviews are positive, and topic modeling shows frequent mention of 'hair', 'fall', and 'growth' together. \"\n",
    "        \"Representative reviews describe visible reduction in hair fall within 3–4 weeks of consistent use.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Is the product sticky or greasy?\",\n",
    "        \"Only about 2–3% of reviews mention 'sticky' or 'oily'. Most of those are mixed to negative, while the majority describe it as lightweight and smooth, \"\n",
    "        \"so users generally find the texture non-sticky.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Does it have a strong fragrance?\",\n",
    "        \"Mentions of fragrance are rare and sentiment is neutral to positive. Users mostly describe the smell as mild or pleasant, \"\n",
    "        \"suggesting fragrance is not a major concern.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How soon do people see results?\",\n",
    "        \"Clustering and keyword patterns highlight '1 month', '3 weeks', and '3 months' as frequent time references. \"\n",
    "        \"Many users reported visible reduction in hair fall and some new growth within the first month of regular use.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Are customers generally satisfied overall?\",\n",
    "        \"Yes. About 65% of VADER sentiment scores and 65% Naive Bayes predictions are positive. \"\n",
    "        \"VADER achieved ≈ 71% accuracy compared to 65% for Naive Bayes, showing that sentiment detection aligns well with overall user satisfaction.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What issues or negatives do users mention?\",\n",
    "        \"Roughly 23% of reviews are negative. The most common complaints include oil leakage during delivery, lack of visible results after multiple bottles, \"\n",
    "        \"and a heavy texture for fine hair. These appear in clusters 1 and 3 from the K-Means grouping.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Can the serum be recommended for consistent users?\",\n",
    "        \"Yes. Representative reviews from the largest clusters highlight that consistent use over 2–3 months leads to reduced hair fall, \"\n",
    "        \"thicker strands, and visible baby-hair growth.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for q, a in questions_answers:\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff00a1-fb9d-4dd8-8721-4cdd2132f7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
